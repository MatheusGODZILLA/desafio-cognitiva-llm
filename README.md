# Desafio T√©cnico - Desenvolvedor Cognitiva Brasil

## Objetivo
Desenvolver uma solu√ß√£o que acesse pelo menos tr√™s plataformas de Modelos de Linguagem de Grande Escala (LLMs) diferentes, gere respostas para uma mesma pergunta e realize uma an√°lise comparativa da qualidade das respostas.

## Tecnologias Utilizadas
- **Framework:** NestJS
- **Linguagem:** TypeScript
- **Documenta√ß√£o:** Swagger
- **APIs Utilizadas:**
  - Gemini (Google Generative AI)
  - DeepSeek (DeepSeek AI via OpenRouter)
  - Llama (Meta AI via OpenRouter)
 
## Configura√ß√£o do Projeto
### Instala√ß√£o das Depend√™ncias
```bash
$ npm install
```
### Execu√ß√£o do Projeto
```bash
# Modo desenvolvimento
$ npm run start

# Modo observador
$ npm run start:dev

# Modo produ√ß√£o
$ npm run start:prod
```
### Execu√ß√£o dos Testes
```bash
# Testes unit√°rios
$ npm run test

# Testes de ponta a ponta (e2e)
$ npm run test:e2e

# Cobertura de testes
$ npm run test:cov
```

## Configura√ß√£o de Vari√°veis de Ambiente
Para executar o projeto localmente, √© necess√°rio configurar um arquivo `.env` com as chaves de API:

```env
GEMINI_API_KEY='SUA_CHAVE_AQUI'
OPENROUTER_API_KEY_1='SUA_CHAVE_AQUI'
OPENROUTER_API_KEY_2='SUA_CHAVE_AQUI'
```

### Como Obter as Chaves de API
- **Google Gemini:** [Obtenha sua API Key aqui](https://aistudio.google.com/app/apikey)
- **OpenRouter (DeepSeek e Llama):** [Obtenha sua API Key aqui](https://openrouter.ai/settings/keys)

## Implementa√ß√£o
A aplica√ß√£o segue uma arquitetura modularizada, garantindo escalabilidade e flexibilidade para futuras implementa√ß√µes. Para garantir um padr√£o unificado na chamada das APIs, foi criada a interface `ILlmProvider`:

```typescript
export interface ILlmProvider {
  generateText(prompt: string): Promise<{ response?: string; error?: string }>;
}
```

Cada modelo foi implementado como um servi√ßo separado:
### Gemini
A API do Gemini foi integrada utilizando o SDK oficial:

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

export class GeminiService implements ILlmProvider {
  private genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '');

  async generateText(prompt: string) {
    const model = this.genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });

    try {
      const result = await model.generateContent(prompt);
      return { response: result.response.text() };
    } catch (error) {
      console.error('Erro na API do Gemini:', error);
      return { error: 'Erro ao gerar resposta' };
    }
  }
}
```

### DeepSeek e Llama (via OpenRouter)
Para acessar os modelos gratuitos DeepSeek V3 e Llama 3.3 70B Instruct, utilizei a OpenRouter:

```typescript
export class DeepSeekService implements ILlmProvider {
  private readonly apiUrl = 'https://openrouter.ai/api/v1/chat/completions';
  private readonly apiKey = process.env.OPENROUTER_API_KEY_1;

  async generateText(
    prompt: string,
  ): Promise<{ response?: string; error?: string }> {
    const requestBody = {
      model: 'deepseek/deepseek-chat:free',
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    };

    try {
      const response = await fetch(this.apiUrl, {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
          // Opcional:
          // 'HTTP-Referer': process.env.YOUR_SITE_URL,
          // 'X-Title': process.env.YOUR_SITE_NAME,
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        return { error: `Erro HTTP ${response.status}: ${errorText}` };
      }

      const result = (await response.json()) as DeepSeekResponse;
      const generatedResponse =
        result.choices?.[0]?.message?.content || 'Nenhuma resposta retornada';
      return { response: generatedResponse };
    } catch (error) {
      console.error('Erro na API do DeepSeek:', error);
      return { error: 'Erro ao gerar resposta pelo DeepSeek' };
    }
  }
}
```

O mesmo modelo de implementa√ß√£o foi utilizado para o Llama, alterando apenas o valor do modelo no `requestBody`:
```typescript
export class LlamaService implements ILlmProvider {
  private readonly apiUrl = 'https://openrouter.ai/api/v1/chat/completions';
  private readonly apiKey = process.env.OPENROUTER_API_KEY_2;

  async generateText(
    prompt: string,
  ): Promise<{ response?: string; error?: string }> {
    const requestBody = {
      model: 'meta-llama/llama-3.3-70b-instruct:free',
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
    };

    try {
      const response = await fetch(this.apiUrl, {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
          // Opcional:
          // 'HTTP-Referer': process.env.YOUR_SITE_URL,
          // 'X-Title': process.env.YOUR_SITE_NAME,
        },
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        return { error: `Erro HTTP ${response.status}: ${errorText}` };
      }

      const result = (await response.json()) as LlamaResponse;
      const generatedResponse =
        result.choices?.[0]?.message?.content || 'Nenhuma resposta retornada';
      return { response: generatedResponse };
    } catch (error) {
      console.error('Erro na API do Llama:', error);
      return { error: 'Erro ao gerar resposta pelo Llama' };
    }
  }
}
```
Embora seja poss√≠vel usar uma √∫nica API Key da OpenRouter para todos os modelos, optei por obter outra de uma conta diferente para contornar os limites de prompts por modelo.

## Fluxo de Funcionamento
1. **Recebimento do Prompt:** O usu√°rio insere um prompt que ser√° processado pelos tr√™s modelos LLMs configurados.
2. **Gera√ß√£o das Respostas:** O servi√ßo `LlmService` chama os m√©todos `generateText` de cada um dos modelos para obter suas respectivas respostas.
3. **Compara√ß√£o das Respostas:** Cada resposta √© avaliada de acordo com quatro crit√©rios:
   - Clareza e coer√™ncia
   - Precis√£o da informa√ß√£o
   - Criatividade ou profundidade da resposta
   - Consist√™ncia gramatical
4. **Autoavalia√ß√£o Assistida por IA:** As respostas s√£o reenviadas aos pr√≥prios modelos para que eles atribuam notas aos crit√©rios definidos.
5. **Determina√ß√£o da Melhor Resposta:** O servi√ßo calcula a m√©dia das notas de cada crit√©rio e escolhe a melhor resposta com base na pontua√ß√£o final.

## Relat√≥rio de Avalia√ß√£o dos Modelos de IA

Ap√≥s a an√°lise das respostas fornecidas pelos modelos **Gemini, DeepSeek e Llama** para diferentes prompts e suas respectivas avalia√ß√µes, √© poss√≠vel destacar algumas observa√ß√µes gerais sobre o desempenho de cada um.

### **1. Observa√ß√µes Gerais**
- **Clareza e Gram√°tica**: O Gemini teve um bom desempenho na gram√°tica em quase todas as respostas, garantindo um texto mais polido e coeso. J√° o DeepSeek se destacou pela clareza em explica√ß√µes mais t√©cnicas, enquanto o Llama teve um desempenho mais inconsistente.
- **Precis√£o das Informa√ß√µes**: O DeepSeek se saiu melhor em precis√£o nas respostas mais t√©cnicas, como a explica√ß√£o de **mec√¢nica qu√¢ntica** e **Projeto Manhattan**, demonstrando um alto n√≠vel de exatid√£o. O Gemini apresentou algumas imprecis√µes nessas √°reas.
- **Criatividade**: A criatividade variou dependendo do tipo de prompt. O DeepSeek teve uma abordagem mais inovadora em sugest√µes de marketing e campanhas publicit√°rias, enquanto o Gemini e o Llama tiveram respostas mais diretas.
- **Melhor Desempenho Geral**: O DeepSeek foi o modelo que mais se destacou ao longo das avalia√ß√µes, conseguindo a maior pontua√ß√£o final em dois prompts cr√≠ticos (**Marketing Natalino** e **Mec√¢nica Qu√¢ntica**). O Gemini tamb√©m teve boas avalia√ß√µes, especialmente em estrutura√ß√£o de respostas, mas pecou em precis√£o. O Llama, por outro lado, teve um desempenho inferior na maioria dos testes, exceto na campanha publicit√°ria, onde obteve a melhor nota.

### **2. Ranking dos Modelos**
1. üèÜ **DeepSeek** ‚Äì Melhor desempenho geral, destacando-se em precis√£o, clareza e criatividade.
2. **Gemini** ‚Äì Forte em estrutura√ß√£o e gram√°tica, mas perdeu pontos na precis√£o de algumas respostas.
3. **Llama** ‚Äì Desempenho mais fraco, apresentando notas inferiores na maioria das categorias.

### **3. Conclus√£o**
- **Se o foco for precis√£o e clareza em temas t√©cnicos**, o **DeepSeek** foi o melhor modelo avaliado.
- **Se o objetivo for respostas bem estruturadas e de f√°cil leitura**, o **Gemini** √© uma boa escolha.
- **Se for necess√°ria uma abordagem mais criativa e diferenciada**, o **Llama** pode ser √∫til em alguns cen√°rios, mas apresentou um desempenho mais inconsistente.

No geral, o DeepSeek foi o modelo que melhor combinou clareza, precis√£o e criatividade, sendo a melhor escolha para a maioria dos casos.

## Conclus√£o
Essa solu√ß√£o foi desenvolvida com foco na escalabilidade e facilidade de adapta√ß√£o para outros modelos de LLMs. Com a implementa√ß√£o da interface `ILlmProvider`, √© poss√≠vel adicionar novos provedores de IA com poucas modifica√ß√µes no c√≥digo.

O pr√≥ximo passo de melhoria seria aprimorar a an√°lise comparativa das respostas, incorporando um sistema de pontua√ß√£o autom√°tico baseado em aprendizado de m√°quina ou m√©tricas qualitativas mais detalhadas.

---

Caso tenha alguma d√∫vida sobre a implementa√ß√£o, fique √† vontade para entrar em contato!

**Desenvolvido por Matheus da Silva**
